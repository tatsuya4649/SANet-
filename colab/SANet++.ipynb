{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SANet++.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6m8w5-2_FBM"
      },
      "source": [
        "・VGG１９のrelu4_1,relu5_1の特徴量を使用(エンコーダーをVGG19に)\n",
        "\n",
        "・loss_cx1,loss_sx3、identity_1x1,identity＿2x50にする\n",
        "\n",
        "・スタイル転送を行う(コンテンツ画像・スタイル画像共にCNN1層)\n",
        "\n",
        "・identity_lossのときはstyle転送用のCNNを行わない\n",
        "\n",
        "・reluなしの3*3convを3つをTransformの足算した後に追加する\n",
        "\n",
        "・スタイル転送時1*1convを1つに変更する\n",
        "\n",
        "・スタイル転送時1*1に変更する\n",
        "\n",
        "・デコーダーの出力サイズをx4倍に変更(完成画像の高画質化)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GQBDIMaeNyG",
        "outputId": "c036c5af-4248-462c-bfdb-b6f43483dda9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEur4l8E8RY0",
        "outputId": "6672643d-a213-46c7-bc85-5bd4e28bc1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Sep 24 03:27:53 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8nPJGFQBj4L"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from matplotlib import  pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRPDohvfG9pd",
        "outputId": "423d7ab4-04eb-4f59-d9e7-bbad7587768c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "device = torch.device('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4C66ffnCxfS"
      },
      "source": [
        "#エンコーダーのセッティング\n",
        "def making_vgg():\n",
        "    vgg = nn.Sequential(\n",
        "    nn.Conv2d(3, 3, (1, 1)),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(3, 64, (3, 3)),\n",
        "    nn.ReLU(),  # relu1-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),  # relu1-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 128, (3, 3)),\n",
        "    nn.ReLU(),  # relu2-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),  # relu2-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU()  # relu5-4\n",
        "    )\n",
        "    vgg.load_state_dict(torch.load(\"/content/drive/My Drive/vgg_normalised.pth\"))\n",
        "    return vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqkbVuxKC018"
      },
      "source": [
        "#デコーダーのセッティング\n",
        "def making_decoder():\n",
        "    decoder = nn.Sequential(\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 32, (3, 3)),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(32, 32, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(32, 16, (3, 3)),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(16, 16, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(16, 3, (3, 3)),\n",
        "    )\n",
        "    decoder.load_state_dict(torch.load(\"/content/drive/My Drive/SANet++/decoder_iter_525000_63.pth\"))\n",
        "    return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f68fULsxDeK9"
      },
      "source": [
        "class SANet(nn.Module):\n",
        "    def __init__(self, in_planes):\n",
        "        super(SANet, self).__init__()\n",
        "        self.f = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        self.g = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        self.h = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        self.sm = nn.Softmax(dim = -1)\n",
        "        self.out_conv = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        \n",
        "        #コンテンツテクスチャ アテンション\n",
        "        #self.sig = nn.Sigmoid()\n",
        "        #self.t = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        #self.glob_ave = nn.AvgPool2d(64)\n",
        "        #スタイルテクスチャ　アテンション\n",
        "        #self.l = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        #self.l_relu = nn.ReLU()\n",
        "\n",
        "    def new_sqrt(self,feat):\n",
        "        return torch.pow(feat,0.5)\n",
        "\n",
        "    def new_var(self,feat):\n",
        "        mu = torch.mean(feat,dim=2)\n",
        "        mu = torch.reshape(x=mu,shape=(mu.shape[0],mu.shape[1],1))\n",
        "        sub = feat - mu\n",
        "        sub = sub * sub\n",
        "        var = torch.mean(sub,dim=2)\n",
        "        shape = feat.shape[2]\n",
        "        var = (shape/(shape - 1))*var\n",
        "        return var\n",
        "\n",
        "    def calc_mean_std(self,feat, eps=1e-5):\n",
        "        # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "        size = feat.size()\n",
        "        assert (len(size) == 4)\n",
        "        N, C = size[:2]\n",
        "        feat_var = self.new_var(feat.view(N, C, -1)) + eps\n",
        "        #feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "        feat_std = self.new_sqrt(feat_var).view(N, C, 1, 1)\n",
        "        #feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "        feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "        return feat_mean, feat_std\n",
        "\n",
        "    def mean_variance_norm(self,feat):\n",
        "        size = feat.size()\n",
        "        mean, std = self.calc_mean_std(feat)\n",
        "        normalized_feat = (feat - mean.expand(size)) / std.expand(size)\n",
        "        return normalized_feat\n",
        "\n",
        "    def forward(self, content, style):\n",
        "        F = self.f(self.mean_variance_norm(content))\n",
        "        G = self.g(self.mean_variance_norm(style))\n",
        "        \n",
        "        #スタイルに関する必要なテクスチャ をアテンションする\n",
        "        #self.glob_ave = nn.AvgPool2d(style.shape[2])\n",
        "        #style_glob_ave_output = self.glob_ave(style)\n",
        "        #l_output = self.l(style_glob_ave_output)\n",
        "        #l_relu_output = self.l_relu(l_output)\n",
        "        #l_sm_output = self.sig(l_relu_output)\n",
        "        #style_texture = style*l_sm_output\n",
        "        \n",
        "        H = self.h(style)\n",
        "        b, c, h, w = F.size()\n",
        "        F = F.view(b, c, w * h).permute(0, 2, 1)\n",
        "        b, c, h, w = G.size()\n",
        "        G = G.view(b, c, w * h)\n",
        "        #F = F[0]\n",
        "        #G = G[0]\n",
        "        S = torch.bmm(F, G)\n",
        "        S = self.sm(S)\n",
        "        b, c, h, w = H.size()\n",
        "        H = H.view(b, c, w * h)\n",
        "        S = S.permute(0, 2, 1)\n",
        "        O = torch.bmm(H, S)\n",
        "        b, c, h, w = content.size()\n",
        "        O = O.view(b, c, h, w)\n",
        "        O = self.out_conv(O)\n",
        "        #コンテンツ に関する必要なテクスチャ をアテンションする\n",
        "        #self.glob_ave = nn.AvgPool2d(content.shape[2])\n",
        "        #glob_ave_output = self.glob_ave(content)\n",
        "        #t_output = self.t(glob_ave_output)\n",
        "        #t_sm_output = self.sig(t_output)\n",
        "        O = O + content\n",
        "        return O"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04CA7BF2AFZW"
      },
      "source": [
        "class VGG_Attention_Content(nn.Module):\n",
        "    def __init__(self,dim):\n",
        "        super().__init__()\n",
        "        #スタイル転送に対するCNN\n",
        "        self.cnn = nn.Conv2d(dim,dim,(1,1))\n",
        "    def forward(self,feats):\n",
        "        #スタイル転送に対するCNN\n",
        "        output = self.cnn(feats)\n",
        "        return output\n",
        "\n",
        "class VGG_Attention_Style(nn.Module):\n",
        "    def __init__(self,dim):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Conv2d(dim,dim,(1,1))\n",
        "    def forward(self,feats):\n",
        "        output = self.cnn(feats)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM07_9TqAMGH"
      },
      "source": [
        "#エンコードされて受け取った特徴マップをスタイル転送用に学習し直す(エンコーダーの質をあげるため)\n",
        "vgg_stylized_conv_5_content = VGG_Attention_Content(dim=512).to(device)\n",
        "vgg_stylized_conv_4_content = VGG_Attention_Content(dim=512).to(device)\n",
        "vgg_stylized_conv_3_content = VGG_Attention_Content(dim=256).to(device)\n",
        "vgg_stylized_conv_2_content = VGG_Attention_Content(dim=128).to(device)\n",
        "vgg_stylized_conv_1_content = VGG_Attention_Content(dim=64).to(device)\n",
        "\n",
        "vgg_stylized_conv_5_style = VGG_Attention_Style(dim=512).to(device)\n",
        "vgg_stylized_conv_4_style = VGG_Attention_Style(dim=512).to(device)\n",
        "vgg_stylized_conv_3_style = VGG_Attention_Style(dim=256).to(device)\n",
        "vgg_stylized_conv_2_style = VGG_Attention_Style(dim=128).to(device)\n",
        "vgg_stylized_conv_1_style = VGG_Attention_Style(dim=64).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8RUbsHQAO6i"
      },
      "source": [
        "class VGG_Stylized_Content(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG_Stylized_Content, self).__init__()\n",
        "        self.vgg_stylized_conv_5 = vgg_stylized_conv_5_content\n",
        "        self.vgg_stylized_conv_4 = vgg_stylized_conv_4_content\n",
        "        self.vgg_stylized_conv_3 = vgg_stylized_conv_3_content\n",
        "        self.vgg_stylized_conv_2 = vgg_stylized_conv_2_content\n",
        "        self.vgg_stylized_conv_1 = vgg_stylized_conv_1_content\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        #sa_output_3 = self.conv3_1(self.conv_pad_3_1(self.ave_pool_3_1(self.sanet3_1(content_feats[-3], style_feats[-3]))))\n",
        "        results = []\n",
        "        for i in range(5):\n",
        "            func = getattr(self, 'vgg_stylized_conv_{:d}'.format(i+1))\n",
        "            results.append(func(inputs[i]))\n",
        "        return results\n",
        "        #return self.merge_conv(self.merge_conv_pad(sa_output_4 + sa_output_5))\n",
        "\n",
        "class VGG_Stylized_Style(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG_Stylized_Style, self).__init__()\n",
        "        self.vgg_stylized_conv_5_style = vgg_stylized_conv_5_style\n",
        "        self.vgg_stylized_conv_4_style = vgg_stylized_conv_4_style\n",
        "        self.vgg_stylized_conv_3_style = vgg_stylized_conv_3_style\n",
        "        self.vgg_stylized_conv_2_style = vgg_stylized_conv_2_style\n",
        "        self.vgg_stylized_conv_1_style = vgg_stylized_conv_1_style\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        #sa_output_3 = self.conv3_1(self.conv_pad_3_1(self.ave_pool_3_1(self.sanet3_1(content_feats[-3], style_feats[-3]))))\n",
        "        results = []\n",
        "        for i in range(5):\n",
        "            func = getattr(self, 'vgg_stylized_conv_{:d}_style'.format(i+1))\n",
        "            results.append(func(inputs[i]))\n",
        "        return results\n",
        "        #return self.merge_conv(self.merge_conv_pad(sa_output_4 + sa_output_5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exnzGglJGfNx"
      },
      "source": [
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "def normal(feat, eps=1e-5):\n",
        "    feat_mean, feat_std= calc_mean_std(feat, eps)\n",
        "    normalized=(feat-feat_mean)/feat_std\n",
        "    return normalized  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8XKLlXb5Abm"
      },
      "source": [
        "class TransformAttention(nn.Module):\n",
        "  def __init__(self,channel):\n",
        "    super().__init__()\n",
        "    self.padding = nn.ReflectionPad2d(1)\n",
        "    self.conv = nn.Conv2d(channel,channel,kernel_size=(3,3))\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  def forward(self,input):\n",
        "    normal_input = normal(input)\n",
        "    padding_output = self.padding(normal_input)\n",
        "    conv_output = self.conv(padding_output)\n",
        "    sigmoid_output = self.sigmoid(conv_output)\n",
        "    return sigmoid_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gkmDDPqC2_S"
      },
      "source": [
        "class Transform(nn.Module):\n",
        "    def __init__(self, in_planes_4,in_planes_5):\n",
        "        super(Transform, self).__init__()\n",
        "        self.sanet4_1 = SANet(in_planes = in_planes_4)\n",
        "        self.sanet5_1 = SANet(in_planes = in_planes_5)\n",
        "        self.upsample5_1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "        self.merge_conv_pad = nn.ReflectionPad2d((1, 1, 1, 1))\n",
        "        self.merge_conv1 = nn.Conv2d(in_planes_5, 512, (3, 3))\n",
        "        self.merge_conv2 = nn.Conv2d(512, 512, (3, 3))\n",
        "        self.merge_conv3 = nn.Conv2d(512, 512, (3, 3))\n",
        "\n",
        "    def forward(self,content4_1, style4_1, content5_1, style5_1):\n",
        "        sa_output_4 = self.sanet4_1(content4_1, style4_1)\n",
        "        sa_output_5 = self.upsample5_1(self.sanet5_1(content5_1, style5_1))\n",
        "        merge_conv_1_output = self.merge_conv1(self.merge_conv_pad(sa_output_4+sa_output_5))\n",
        "        merge_conv_2_output = self.merge_conv2(self.merge_conv_pad(merge_conv_1_output))\n",
        "        merge_conv_3_output = self.merge_conv3(self.merge_conv_pad(merge_conv_2_output))\n",
        "        return merge_conv_3_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S15sAgGCAaSx"
      },
      "source": [
        "def making_vgg_stylized():\n",
        "    #vgg_stylized = VGG_Stylized()\n",
        "    #vgg_stylized.load_state_dict(torch.load(\"/content/drive/My Drive/SANetPytorch/vgg_stylized_iter_100000_7.pth\"))\n",
        "    vgg_stylized_content = VGG_Stylized_Content()\n",
        "    vgg_stylized_style = VGG_Stylized_Style()\n",
        "    vgg_stylized_content.load_state_dict(torch.load(\"/content/drive/My Drive/SANet++/vgg_stylized_content_iter_525000_63.pth\"))\n",
        "    vgg_stylized_style.load_state_dict(torch.load(\"/content/drive/My Drive/SANet++/vgg_stylized_style_iter_525000_63.pth\"))\n",
        "    return vgg_stylized_content,vgg_stylized_style\n",
        "    #return vgg_stylized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDlv1hb1DWfR"
      },
      "source": [
        "def making_sanet():\n",
        "    sanet = Transform(512,512)\n",
        "    sanet.load_state_dict(torch.load(\"/content/drive/My Drive/SANet++/sa_module_iter_525000_63.pth\"))\n",
        "    return sanet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tq--yeUGT7x",
        "outputId": "731e01c4-bd03-4370-e639-cf327472f693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "vgg = making_vgg()\n",
        "vgg.to(device)\n",
        "vgg_stylized_content,vgg_stylized_style = making_vgg_stylized()\n",
        "sa_module = making_sanet()\n",
        "decoder = making_decoder()\n",
        "decoder.eval().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (2): ReLU()\n",
              "  (3): Upsample(scale_factor=2.0, mode=nearest)\n",
              "  (4): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (6): ReLU()\n",
              "  (7): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (9): ReLU()\n",
              "  (10): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (12): ReLU()\n",
              "  (13): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (14): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (15): ReLU()\n",
              "  (16): Upsample(scale_factor=2.0, mode=nearest)\n",
              "  (17): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (18): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (19): ReLU()\n",
              "  (20): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (21): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (22): ReLU()\n",
              "  (23): Upsample(scale_factor=2.0, mode=nearest)\n",
              "  (24): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (25): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (26): ReLU()\n",
              "  (27): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (28): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (29): Upsample(scale_factor=2.0, mode=nearest)\n",
              "  (30): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (31): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (32): ReLU()\n",
              "  (33): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (34): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (35): Upsample(scale_factor=2.0, mode=nearest)\n",
              "  (36): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (37): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (38): ReLU()\n",
              "  (39): ReflectionPad2d((1, 1, 1, 1))\n",
              "  (40): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi49ZJ7NfccO"
      },
      "source": [
        "def big_images(images):\n",
        "  rcan_output = rcan(images)\n",
        "  return rcan_output\n",
        "\n",
        "def small_images(images):\n",
        "  output = torch.nn.functional.interpolate(images,scale_factor=0.25,mode=\"bicubic\")\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79L6Onc6FAjf"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,encoder , decoder):\n",
        "        super(Net, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        enc_layers = list(encoder.children())\n",
        "        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n",
        "        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
        "        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
        "        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
        "        self.enc_5 = nn.Sequential(*enc_layers[31:44])  # relu4_1 -> relu5_1\n",
        "        #transform\n",
        "        self.sa_module = sa_module\n",
        "        self.upsampling = nn.Upsample(32)\n",
        "        self.vgg_stylized_content = vgg_stylized_content\n",
        "        self.vgg_stylized_style = vgg_stylized_style\n",
        "        self.decoder = decoder\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.variation_loss = nn.L1Loss()\n",
        "        # fix the encoder\n",
        "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4', 'enc_5']:\n",
        "            for param in getattr(self, name).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # extract relu1_1, relu2_1, relu3_1, relu4_1, relu5_1 from input image\n",
        "    def encode_with_intermediate(self, input):\n",
        "        results = [input]\n",
        "        for i in range(5):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "    def calc_content_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        #assert (target.requires_grad is False)\n",
        "        return self.mse_loss(input, target)\n",
        "\n",
        "    def calc_style_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        #assert (target.requires_grad is False)\n",
        "        input_mean, input_std = calc_mean_std(input)\n",
        "        target_mean, target_std = calc_mean_std(target)\n",
        "        return self.mse_loss(input_mean, target_mean) + \\\n",
        "               self.mse_loss(input_std, target_std)\n",
        "    \n",
        "    def compute_total_variation_loss_l1(self, inputs):\n",
        "        h = inputs.shape[2]\n",
        "        w = inputs.shape[3]\n",
        "        h1 = inputs[:, :, 0:h-1, :]\n",
        "        h2 = inputs[:, :, 1:h, :]\n",
        "        w1 = inputs[:, :, :, 0:w-1]\n",
        "        w2 = inputs[:, :, :, 1:w]\n",
        "        return self.variation_loss(h1, h2)+self.variation_loss(w1, w2)\n",
        "\n",
        "    def generate_image(self,content,style):\n",
        "        style_feats = self.encode_with_intermediate(style)\n",
        "        content_feats = self.encode_with_intermediate(content)\n",
        "        new_style_feats = self.vgg_stylized_style(style_feats)\n",
        "        new_content_feats = self.vgg_stylized_content(content_feats)\n",
        "        sanet_output = self.sa_module(new_content_feats[-2],new_style_feats[-2],new_content_feats[-1],new_style_feats[-1])\n",
        "        decoder_input = sanet_output\n",
        "        Ics = self.decoder(decoder_input)\n",
        "        return Ics\n",
        "    \n",
        "    def forward(self, content, style):\n",
        "        style_feats = self.encode_with_intermediate(style)\n",
        "        content_feats = self.encode_with_intermediate(content)\n",
        "        new_style_feats = self.vgg_stylized_style(style_feats)\n",
        "        new_content_feats = self.vgg_stylized_content(content_feats)\n",
        "        #スタイル変換用\n",
        "        sanet_output = self.sa_module(new_content_feats[-2],new_style_feats[-2],new_content_feats[-1],new_style_feats[-1])\n",
        "        decoder_input = sanet_output\n",
        "        Ics = self.decoder(decoder_input)\n",
        "        Ics = small_images(Ics)\n",
        "        Ics_feats = self.encode_with_intermediate(Ics)\n",
        "        # Variation loss\n",
        "        loss_v = self.compute_total_variation_loss_l1(Ics)\n",
        "        # Content loss\n",
        "        loss_c = self.calc_content_loss(normal(Ics_feats[0]), normal(content_feats[0]))+self.calc_content_loss(normal(Ics_feats[1]), normal(content_feats[1]))+self.calc_content_loss(normal(Ics_feats[2]), normal(content_feats[2]))+self.calc_content_loss(normal(Ics_feats[3]), normal(content_feats[3]))+self.calc_content_loss(normal(Ics_feats[4]), normal(content_feats[4]))\n",
        "        # Style loss\n",
        "        loss_s = self.calc_style_loss(Ics_feats[0], style_feats[0])+self.calc_style_loss(Ics_feats[1], style_feats[1])+self.calc_style_loss(Ics_feats[2], style_feats[2])+self.calc_style_loss(Ics_feats[3], style_feats[3])+self.calc_style_loss(Ics_feats[4], style_feats[4])\n",
        "\n",
        "        #new_style_feats_content = self.vgg_stylized_content(style_feats)\n",
        "        #new_content_feats_style = self.vgg_stylized_style(content_feats)\n",
        "        #Identity losses lambda 1\n",
        "        Icc = self.decoder(self.sa_module(content_feats[-2],content_feats[-2],content_feats[-1],content_feats[-1]))\n",
        "        Iss = self.decoder(self.sa_module(style_feats[-2],style_feats[-2],style_feats[-1],style_feats[-1])) \n",
        "\n",
        "        Icc = small_images(Icc)\n",
        "        Iss = small_images(Iss)\n",
        "        \n",
        "        loss_lambda1 = self.calc_content_loss(Icc,content)+self.calc_content_loss(Iss,style)\n",
        "        \n",
        "        #Identity losses lambda 2\n",
        "        Icc_feats=self.encode_with_intermediate(Icc)\n",
        "        Iss_feats=self.encode_with_intermediate(Iss)\n",
        "        loss_lambda2 = self.calc_content_loss(Icc_feats[0], content_feats[0])+self.calc_content_loss(Iss_feats[0], style_feats[0])\n",
        "        for i in range(1, 5):\n",
        "            loss_lambda2 += self.calc_content_loss(Icc_feats[i], content_feats[i])+self.calc_content_loss(Iss_feats[i], style_feats[i])\n",
        "        return loss_c, loss_s, loss_lambda1, loss_lambda2,loss_v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XixPLvsvfkhh"
      },
      "source": [
        "### 訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2kDgvXxGowW",
        "outputId": "d2daed72-cfbf-4c63-9f6a-b4bf6b7824cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "network = Net(vgg,decoder)\n",
        "network.train()\n",
        "network.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (encoder): Sequential(\n",
              "    (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (8): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (10): ReLU()\n",
              "    (11): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (15): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (17): ReLU()\n",
              "    (18): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (20): ReLU()\n",
              "    (21): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (23): ReLU()\n",
              "    (24): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (26): ReLU()\n",
              "    (27): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (28): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (29): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (30): ReLU()\n",
              "    (31): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (33): ReLU()\n",
              "    (34): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (36): ReLU()\n",
              "    (37): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (38): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (39): ReLU()\n",
              "    (40): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (41): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (43): ReLU()\n",
              "    (44): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (45): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (46): ReLU()\n",
              "    (47): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (48): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (49): ReLU()\n",
              "    (50): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (51): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (52): ReLU()\n",
              "  )\n",
              "  (enc_1): Sequential(\n",
              "    (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (enc_2): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (4): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (6): ReLU()\n",
              "  )\n",
              "  (enc_3): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (4): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (6): ReLU()\n",
              "  )\n",
              "  (enc_4): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (10): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (12): ReLU()\n",
              "  )\n",
              "  (enc_5): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (10): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (12): ReLU()\n",
              "  )\n",
              "  (sa_module): Transform(\n",
              "    (sanet4_1): SANet(\n",
              "      (f): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (g): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (h): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (sm): Softmax(dim=-1)\n",
              "      (out_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (sanet5_1): SANet(\n",
              "      (f): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (g): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (h): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (sm): Softmax(dim=-1)\n",
              "      (out_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (upsample5_1): Upsample(scale_factor=2.0, mode=nearest)\n",
              "    (merge_conv_pad): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (merge_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (merge_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (merge_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  )\n",
              "  (upsampling): Upsample(size=32, mode=nearest)\n",
              "  (vgg_stylized_content): VGG_Stylized_Content(\n",
              "    (vgg_stylized_conv_5): VGG_Attention_Content(\n",
              "      (cnn): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (vgg_stylized_conv_4): VGG_Attention_Content(\n",
              "      (cnn): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (vgg_stylized_conv_3): VGG_Attention_Content(\n",
              "      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (vgg_stylized_conv_2): VGG_Attention_Content(\n",
              "      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (vgg_stylized_conv_1): VGG_Attention_Content(\n",
              "      (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (vgg_stylized_style): VGG_Stylized_Style(\n",
              "    (vgg_stylized_conv_5_style): VGG_Attention_Style(\n",
              "      (cnn): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (vgg_stylized_conv_4_style): VGG_Attention_Style(\n",
              "      (cnn): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (vgg_stylized_conv_3_style): VGG_Attention_Style(\n",
              "      (cnn): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (vgg_stylized_conv_2_style): VGG_Attention_Style(\n",
              "      (cnn): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (vgg_stylized_conv_1_style): VGG_Attention_Style(\n",
              "      (cnn): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): Upsample(scale_factor=2.0, mode=nearest)\n",
              "    (4): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (9): ReLU()\n",
              "    (10): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (12): ReLU()\n",
              "    (13): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (14): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (15): ReLU()\n",
              "    (16): Upsample(scale_factor=2.0, mode=nearest)\n",
              "    (17): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (18): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (19): ReLU()\n",
              "    (20): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (21): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (22): ReLU()\n",
              "    (23): Upsample(scale_factor=2.0, mode=nearest)\n",
              "    (24): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (25): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (26): ReLU()\n",
              "    (27): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (28): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (29): Upsample(scale_factor=2.0, mode=nearest)\n",
              "    (30): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (31): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (32): ReLU()\n",
              "    (33): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (34): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (35): Upsample(scale_factor=2.0, mode=nearest)\n",
              "    (36): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (37): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (38): ReLU()\n",
              "    (39): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (40): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1))\n",
              "  )\n",
              "  (mse_loss): MSELoss()\n",
              "  (variation_loss): L1Loss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESsWXSwHG2Zt"
      },
      "source": [
        "optimizer = torch.optim.Adam([{'params': network.decoder.parameters()},{'params': network.sa_module.parameters()},{'params': network.vgg_stylized_content.parameters()},{'params': network.vgg_stylized_style.parameters()}], lr=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv0xKOgrHqnG"
      },
      "source": [
        "def adjust_learning_rate(optimizer, iteration_count):\n",
        "    \"\"\"Imitating the original implementation\"\"\"\n",
        "    lr = 1e-4 / (1.0 + 5e-5 * iteration_count)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EjjTUr-IMG9"
      },
      "source": [
        "MAX_ITER = 1000000\n",
        "SAVE_MODEL_INTERVAL = 5000\n",
        "BATCH_SIZE = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY5BiRpgJ8NT"
      },
      "source": [
        "def getting_coco_dataset(i):\n",
        "  j = i * 1000 + 1000\n",
        "  files = glob(\"/content/drive/My Drive/MSCOCO_torch_training_256_new/*_{}.pickle\".format(j))\n",
        "  file = files[0]\n",
        "  print(file)\n",
        "  with open(file=file,mode=\"rb\") as f:\n",
        "    image_arrays = pickle.load(file=f)\n",
        "  return image_arrays\n",
        "\n",
        "def getting_wikiart_dataset(i):\n",
        "  j = i * 1000 + 1000\n",
        "  files = glob(\"/content/drive/My Drive/WIKIART_torch_training/*_{}.pickle\".format(j))\n",
        "  file = files[0]\n",
        "  print(file)\n",
        "  with open(file=file,mode=\"rb\") as f:\n",
        "    image_arrays = pickle.load(file=f)\n",
        "  return image_arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNupbYi3ES47"
      },
      "source": [
        "loss_array = []\n",
        "loss_c_array = []\n",
        "loss_s_array = []\n",
        "loss_identity_1_array = []\n",
        "loss_identity_2_array = []\n",
        "loss_v_array = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHk4p4rKMDK6"
      },
      "source": [
        "def test_image(content,style):\n",
        "    coco_file = content\n",
        "    coco=Image.open(coco_file)\n",
        "    coco = coco.convert('RGB')\n",
        "    _DEFAULT_SIZE = 512\n",
        "    if coco.width > coco.height:\n",
        "        p = torchvision.transforms.Compose([\n",
        "                                        torchvision.transforms.Resize((int(_DEFAULT_SIZE*(coco.width/coco.height)),_DEFAULT_SIZE)),\n",
        "                                        torchvision.transforms.RandomCrop(512)\n",
        "                                        ])\n",
        "    else:\n",
        "        p = torchvision.transforms.Compose([\n",
        "                                        torchvision.transforms.Resize((_DEFAULT_SIZE,int(_DEFAULT_SIZE*(coco.height/coco.width)))),\n",
        "                                        torchvision.transforms.RandomCrop(512)\n",
        "                                        ])\n",
        "    coco_tensor = torchvision.transforms.functional.to_tensor(p(coco))\n",
        "    coco_tensor = torch.unsqueeze(coco_tensor,dim=0).to(device)\n",
        "    wiki_file = style\n",
        "    wiki=Image.open(wiki_file)\n",
        "    wiki = wiki.convert('RGB')\n",
        "    if wiki.width > wiki.height:\n",
        "        p = torchvision.transforms.Compose([\n",
        "                                        torchvision.transforms.Resize((int(_DEFAULT_SIZE*(wiki.width/wiki.height)),_DEFAULT_SIZE)),\n",
        "                                        torchvision.transforms.RandomCrop(512)\n",
        "                                        ])\n",
        "    else:\n",
        "        p = torchvision.transforms.Compose([\n",
        "                                        torchvision.transforms.Resize((_DEFAULT_SIZE,int(_DEFAULT_SIZE*(wiki.height/wiki.width)))),\n",
        "                                        torchvision.transforms.RandomCrop(512)\n",
        "                                        ])\n",
        "    wiki_tensor = torchvision.transforms.functional.to_tensor(p(wiki))\n",
        "    wiki_tensor = torch.unsqueeze(wiki_tensor,dim=0).to(device)\n",
        "    decoder_output = network.generate_image(coco_tensor,wiki_tensor)\n",
        "    image = decoder_output[0].cpu()\n",
        "    image = image.detach().numpy()\n",
        "    generated_imge = image\n",
        "    image = np.transpose(image, (1,2,0))\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1EGGgy08tcC"
      },
      "source": [
        "test_image(\"/content/brad_pitt.jpg\",\"/content/023.jpg\")\n",
        "test_image(\"/content/brad_pitt.jpg\",\"/content/la_muse.jpg\")\n",
        "test_image(\"/content/brad_pitt.jpg\",\"/content/impronte_d_artista.jpg\")\n",
        "test_image(\"/content/brad_pitt.jpg\",\"/content/fire.jpg\")\n",
        "test_image(\"/content/brad_pitt.jpg\",\"/content/goph.jpg\")\n",
        "test_image(\"/content/brad_pitt.jpg\",\"/content/in1.jpg\")\n",
        "test_image(\"/content/brad_pitt.jpg\",\"/content/in2.jpg\")\n",
        "test_image(\"/content/brad_pitt.jpg\",\"/content/in3.jpg\")\n",
        "test_image(\"/content/brad_pitt.jpg\",\"/content/in4.jpg\")\n",
        "\n",
        "test_image(\"/content/cat.jpg\",\"/content/023.jpg\")\n",
        "test_image(\"/content/cat.jpg\",\"/content/la_muse.jpg\")\n",
        "test_image(\"/content/cat.jpg\",\"/content/impronte_d_artista.jpg\")\n",
        "test_image(\"/content/cat.jpg\",\"/content/fire2.jpg\")\n",
        "test_image(\"/content/cat.jpg\",\"/content/goph.jpg\")\n",
        "test_image(\"/content/cat.jpg\",\"/content/in1.jpg\")\n",
        "test_image(\"/content/cat.jpg\",\"/content/in2.jpg\")\n",
        "test_image(\"/content/cat.jpg\",\"/content/in3.jpg\")\n",
        "test_image(\"/content/cat.jpg\",\"/content/in4.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RyaVP7Ie3gr"
      },
      "source": [
        "for i in tqdm(range(525000,MAX_ITER)):\n",
        "  #1000/BATCH_SIZEに1度呼び出して、画像データを入れ替える\n",
        "  if i % int(1000/BATCH_SIZE) == 0:\n",
        "    if i != 0:\n",
        "      count = int(int((i)*BATCH_SIZE/1000) % 80)\n",
        "      #Tensor型配列(要素数1000)が入っている\n",
        "      content_images_datasets = getting_coco_dataset(count)\n",
        "      style_images_datasets = getting_wikiart_dataset(count)\n",
        "    elif i == 0:\n",
        "      count = 0\n",
        "      #Tensor型配列(要素数1000)が入っている\n",
        "      content_images_datasets = getting_coco_dataset(count)\n",
        "      style_images_datasets = getting_wikiart_dataset(count)\n",
        "  now_count = i % int(1000/BATCH_SIZE)\n",
        "  content_images = content_images_datasets[now_count:now_count+BATCH_SIZE]\n",
        "  style_images = style_images_datasets[now_count:now_count+BATCH_SIZE]\n",
        "  content_images = torch.cat(content_images).reshape(len(content_images), *content_images[0].shape)\n",
        "  style_images = torch.cat(style_images).reshape(len(style_images), *style_images[0].shape)\n",
        "  content_images = content_images.to(device)\n",
        "  style_images = style_images.to(device)\n",
        "  #adjust_learning_rate(optimizer, iteration_count=i)\n",
        "  loss_c, loss_s, l_identity1, l_identity2,loss_v = network(content_images, style_images)\n",
        "  loss = 1 * loss_c + loss_s * 3 + (l_identity1 * 1) + (l_identity2 * 50) #+ loss_v * 20\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if (i + 1) % 1000 == 0:\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/023.jpg\")\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/la_muse.jpg\")\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/impronte_d_artista.jpg\")\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/fire.jpg\")\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/goph.jpg\")\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/in1.jpg\")\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/in2.jpg\")\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/in3.jpg\")\n",
        "    test_image(\"/content/brad_pitt.jpg\",\"/content/in4.jpg\")\n",
        "\n",
        "    test_image(\"/content/cat.jpg\",\"/content/023.jpg\")\n",
        "    test_image(\"/content/cat.jpg\",\"/content/la_muse.jpg\")\n",
        "    test_image(\"/content/cat.jpg\",\"/content/impronte_d_artista.jpg\")\n",
        "    test_image(\"/content/cat.jpg\",\"/content/fire2.jpg\")\n",
        "    test_image(\"/content/cat.jpg\",\"/content/goph.jpg\")\n",
        "    test_image(\"/content/cat.jpg\",\"/content/in1.jpg\")\n",
        "    test_image(\"/content/cat.jpg\",\"/content/in2.jpg\")\n",
        "    test_image(\"/content/cat.jpg\",\"/content/in3.jpg\")\n",
        "    test_image(\"/content/cat.jpg\",\"/content/in4.jpg\")\n",
        "\n",
        "  if (i + 1) % 100 == 0:\n",
        "    loss_c_array.append(loss_c.data.item())\n",
        "    loss_s_array.append(loss_s.data.item())\n",
        "    loss_identity_1_array.append(l_identity1.data.item())\n",
        "    loss_identity_2_array.append(l_identity2.data.item())\n",
        "    loss_array.append(loss.data.item())\n",
        "    count = len(loss_array)\n",
        "    x = np.arange(count)\n",
        "    plt.plot(x, loss_array, label=\"loss\")\n",
        "    plt.show()\n",
        "\n",
        "  if (i + 1) % SAVE_MODEL_INTERVAL == 0 or (i + 1) == MAX_ITER:\n",
        "        state_dict = network.decoder.state_dict()\n",
        "        for key in state_dict.keys():\n",
        "            state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
        "        torch.save(state_dict,'/content/drive/My Drive/SANet++/decoder_iter_{}_63.pth'.format(i + 1))\n",
        "        state_dict = network.sa_module.state_dict()\n",
        "        for key in state_dict.keys():\n",
        "            state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
        "        torch.save(state_dict,'/content/drive/My Drive/SANet++/sa_module_iter_{}_63.pth'.format(i + 1))\n",
        "        state_dict = network.vgg_stylized_content.state_dict()\n",
        "        for key in state_dict.keys():\n",
        "            state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
        "        torch.save(state_dict,'/content/drive/My Drive/SANet++/vgg_stylized_content_iter_{}_63.pth'.format(i + 1))\n",
        "        state_dict = network.vgg_stylized_style.state_dict()\n",
        "        for key in state_dict.keys():\n",
        "            state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
        "        torch.save(state_dict,'/content/drive/My Drive/SANet++/vgg_stylized_style_iter_{}_63.pth'.format(i + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2TTVJnPxNfF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}